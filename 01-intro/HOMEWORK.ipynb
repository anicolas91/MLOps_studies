{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The homework looks into the YELLOW taxi records for January and february of 2023, so we will use those instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Downloading the data. \n",
    "For the january data of 2023 yellow taxis, how many columns are there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns:  19\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('./data/yellow_tripdata_2023-01.parquet')\n",
    "print('Number of columns: ', len(df.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Computing duration\n",
    "What is the standard deviation of the trips duration in january?\n",
    "We calculate the duration of each trip in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The standard deviation of the trips before we downselect is: 42.59\n"
     ]
    }
   ],
   "source": [
    "df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "print(f'The standard deviation of the trips before we downselect is: {df.duration.std():.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Dropping outliers\n",
    "We will filter so that we only keep data betwen 1 and 60 mins (inclusive).\n",
    "What fraction of the recorsd are left after you dropped the outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage left after dropping outliers is 98%\n"
     ]
    }
   ],
   "source": [
    "mask = ((df.duration >=1) & (df.duration<=60))\n",
    "df = df[mask]\n",
    "total_n = len(mask)\n",
    "filtered_n = mask.sum()\n",
    "print(f'percentage left after dropping outliers is {filtered_n/total_n*100:.0f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. One hot encoding\n",
    "Let's apply one-hot encoding to the pickup and dropoff location IDs. We'll use only these two features for our model.\n",
    "\n",
    "- Turn the dataframe into a list of dictionaries (remember to re-cast the ids to strings - otherwise it will label encode them)\n",
    "- Fit a dictionary vectorizer\n",
    "- Get a feature matrix from it\n",
    "   \n",
    "What's the dimensionality of this matrix (number of columns)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['PULocationID', 'DOLocationID'] # pickup and dropoff location\n",
    "numerical = ['trip_distance'] # distance of trip\n",
    "df[categorical] = df[categorical].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do one hot encoding (converting things to 0s and 1s).\n",
    "First we will convert the categoricals to just objects.\n",
    "The reason to make the categoricals strings is because otherwise the dataframe will label encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical].dtypes #object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert each row to a dictionary, and we will use the dictionary vectorizer to convert the dictionary to just vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dicts = df[categorical + numerical].to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dicts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DictVectorizer()\n",
    "X_train = dv.fit_transform(train_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can check the no of columns and the feature names for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv.feature_names_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'no of columns on one hot: {X_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the X variables, we also need to set the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'duration'\n",
    "y_train = df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the x and y for training, we can fit a model to this.\n",
    "Remember, this is basically:\n",
    "duration = f(dropoff location, pickup location, trip distance)\n",
    "\n",
    "We will fit a simple linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will predict the duration based on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_pred,label='prediction')\n",
    "sns.distplot(y_train,label='actual')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction and the actual values look pretty different. This means that the model is probably not particularly good. We can calcualte the performance of this model using the root mean square error RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse = root_mean_squared_error(y_train,y_pred)\n",
    "print(f'RMSE from train data is : {train_rmse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should check now how it performs with the validation dataset. We will create a function that we can use to preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(filename):\n",
    "    df = pd.read_parquet(filename) #read data\n",
    "    df['duration'] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime # calculate duration\n",
    "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60) # convert to mins\n",
    "    mask = ((df.duration >=1) & (df.duration<=60)) # select only significant data\n",
    "    df = df[mask]\n",
    "    categorical = ['PULocationID', 'DOLocationID'] # pickup and dropoff location\n",
    "    df[categorical] = df[categorical].astype(str) # convert to string/object to prevent labeling\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read in both the training and the validation data using this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_dataframe('./data/green_tripdata_2023-01.parquet')\n",
    "df_val = read_dataframe('./data/green_tripdata_2023-02.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the length of each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train),len(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the one-hotting and calculate RMSE. Note that it got worse, and if looking at the video, lasso and ridge models actually do not fare any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DictVectorizer()\n",
    "lr = LinearRegression()\n",
    "categorical = ['PULocationID', 'DOLocationID'] # pickup and dropoff location\n",
    "numerical = ['trip_distance'] # distance of trip\n",
    "\n",
    "train_dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "val_dicts = df_val[categorical + numerical].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dicts) # no need to fit sicne its done already for training data\n",
    "\n",
    "y_train = df_train['duration'].values\n",
    "y_val = df_val['duration'].values\n",
    "\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "y_pred_train = lr.predict(X_train)\n",
    "y_pred_val   = lr.predict(X_val)\n",
    "\n",
    "train_rmse = root_mean_squared_error(y_train,y_pred_train)\n",
    "val_rmse   = root_mean_squared_error(y_val,y_pred_val)\n",
    "\n",
    "print(f'RMSE from train data is : {train_rmse:.2f}')\n",
    "print(f'RMSE from validation data is : {val_rmse:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve things, we combine the dropoff/pickup locations into one variable and repeat the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('combining pickup location ID and dropoff location ID...')\n",
    "df_train['PU_DO'] = df_train['PULocationID'] + '_' + df_train['DOLocationID']\n",
    "df_val['PU_DO'] = df_val['PULocationID'] + '_' + df_val['DOLocationID']\n",
    "\n",
    "dv = DictVectorizer()\n",
    "lr = LinearRegression()\n",
    "categorical = ['PU_DO'] # pickup and dropoff location\n",
    "numerical = ['trip_distance'] # distance of trip\n",
    "\n",
    "train_dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "val_dicts = df_val[categorical + numerical].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dicts) # no need to fit sicne its done already for training data\n",
    "\n",
    "y_train = df_train['duration'].values\n",
    "y_val = df_val['duration'].values\n",
    "\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "y_pred_train = lr.predict(X_train)\n",
    "y_pred_val   = lr.predict(X_val)\n",
    "\n",
    "train_rmse = root_mean_squared_error(y_train,y_pred_train)\n",
    "val_rmse   = root_mean_squared_error(y_val,y_pred_val)\n",
    "\n",
    "print(f'RMSE from train data is : {train_rmse:.2f}')\n",
    "print(f'RMSE from validation data is : {val_rmse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression works best for the data. Lasso and ridge tend to fare same or worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_lasso = Lasso(0.001)\n",
    "lr_lasso.fit(X_train,y_train)\n",
    "\n",
    "y_pred_train = lr_lasso.predict(X_train)\n",
    "y_pred_val   = lr_lasso.predict(X_val)\n",
    "\n",
    "train_rmse = root_mean_squared_error(y_train,y_pred_train)\n",
    "val_rmse   = root_mean_squared_error(y_val,y_pred_val)\n",
    "\n",
    "print(f'RMSE from train data using Lasso is : {train_rmse:.2f}')\n",
    "print(f'RMSE from validation data using Lasso is : {val_rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_ridge = Ridge(alpha=0.001)\n",
    "lr_ridge.fit(X_train,y_train)\n",
    "\n",
    "y_pred_train = lr_ridge.predict(X_train)\n",
    "y_pred_val   = lr_ridge.predict(X_val)\n",
    "\n",
    "train_rmse = root_mean_squared_error(y_train,y_pred_train)\n",
    "val_rmse   = root_mean_squared_error(y_val,y_pred_val)\n",
    "\n",
    "print(f'RMSE from train data using Ridge is : {train_rmse:.2f}')\n",
    "print(f'RMSE from validation data using Ridge is : {val_rmse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that lasso takes the longest because we're looking at 60 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save the model using pickle and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/lin_reg.bin', 'wb') as f_out:\n",
    "    pickle.dump((dv, lr), f_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
